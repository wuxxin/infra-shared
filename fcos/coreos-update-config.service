[Unit]
Description=Update Coreos Configuration by applying (transpiled butane) saltstack config
After=boot-complete.target
Wants=boot-complete.target
ConditionPathExists=/run/user/1000/%N/sls/main.sls

# Run config update by
# building saltstack in a fedora container,
# executing salt-call in this container,
# mounting /etc, /var, /run from host into container, /etc is also mounted as /host_etc
# butane2salt.jinja remaps files /etc/[hosts,hostname,resolv.conf]# to /host_etc/*

[Service]
Type=oneshot

# create config for saltstack container
ExecStart=/usr/bin/bash -c 'mkdir -p /etc/containers/build/%N && printf \
"FROM docker.io/library/fedora:39\
\nRUN dnf -y install salt salt-minion lsb-release && rm -rf /var/cache/* && dnf clean all && rm -rf /var/tmp/*\
\nCMD lsb_release -a\
\n" > /etc/containers/build/%N/Containerfile'

# build saltstack container
Environment=PODMAN_BUILD_OPTIONS=
ExecStart=/usr/bin/bash -c 'cd /etc/containers/build/%N && \
/usr/bin/podman build $PODMAN_BUILD_OPTIONS --tag localhost/%N:latest . '

# roughly equal to tools.py:salt_config
Environment=resource_name="%n"
Environment=base_dir=/run/user/1000
Environment=root_dir=/run/user/1000/%N
Environment=tmp_dir=/run/user/1000/%N/tmp
Environment=sls_dir=/run/user/1000/%N/sls
Environment=pillar_dir=/run/user/1000/%N/pillar

# create a minion config
ExecStart=/usr/bin/bash -c \
'printf "id: ${resource_name}\
\nlocal: True\
\nlog_level_logfile: info\
\nfile_client: local\
\nfileserver_backend:\
\n- roots\
\nfile_roots:\
\n  base:\
\n  - ${sls_dir}\
\npillar_roots:\
\n  base:\
\n  - ${pillar_dir}\
\ngrains:\
\n  resource_name: ${resource_name}\
\n  base_dir: ${base_dir}\
\n  root_dir: ${root_dir}\
\n  tmp_dir: ${tmp_dir}\
\n  sls_dir: ${sls_dir}\
\n  pillar_dir: ${pillar_dir}\
\nroot_dir: ${root_dir}\
\nconf_file: ${root_dir}/minion\
\npki_dir: ${root_dir}/etc/salt/pki/minion\
\npidfile: ${root_dir}/var/run/salt-minion.pid\
\nsock_dir: ${root_dir}/var/run/salt/minion\
\ncachedir: ${root_dir}/var/cache/salt/minion\
\nextension_modules: ${root_dir}/var/cache/salt/minion/extmods\
\nlog_file: ${root_dir}/var/log/salt/minion\
\n" > ${root_dir}/minion'

# create sls/top.sls, pillar/top.sls, pillar/main.sls
ExecStart=/usr/bin/bash -c \
    'mkdir -p "${sls_dir}" "${pillar_dir}"; \
    printf "base:\\n  \\"*\\":\\n    - main\\n" > "${sls_dir}/top.sls"; \
    printf "base:\\n  \\"*\\":\\n    - main\\n" > "${pillar_dir}/top.sls"; \
    printf "resource_name: ${resource_name}\\n" > "${pillar_dir}/main.sls"'

# XXX use logdriver=none, to prevent duplicate logging in journald
# make a salt-call test/mock run, to see if update would abort midways
ExecStart=/usr/bin/podman run --rm --privileged --log-driver=none \
    -v /etc:/etc -v /etc:/host_etc -v /var:/var -v /run:/run -v /var/home:/home -v /var/roothome:/root \
    localhost/%N:latest \
    /usr/bin/salt-call -c ${root_dir} state.highstate mock=true

# do the actual update work
ExecStart=/usr/bin/podman run --rm --privileged --log-driver=none \
    -v /etc:/etc -v /etc:/host_etc -v /var:/var -v /run:/run -v /var/home:/home -v /var/roothome:/root \
    localhost/%N:latest \
    /usr/bin/salt-call -c ${root_dir} state.highstate

# reload systemd daemon to resync with state from disk
ExecStart=/usr/bin/systemctl daemon-reload

# enable and disable each service that should be enabled/disabled, by list created from butane2salt.jinja
ExecStart=/usr/bin/bash -c '\
    for status in enable disable; do \
        if test -e ${root_dir}/service_$status.req; then \
            service_list=$(cat ${root_dir}/service_$status.req | \
                grep -v "^#" | grep -v "^[[:space:]]*$" | sort | uniq); \
            if test "$service_list" != ""; then \
                echo "systemctl $status $service_list"; \
                systemctl $status $service_list; \
            fi; \
        fi; \
    done'

# reset failed units and restart each service that got changed, by list created from butane2salt.jinja
# dont act on template services, dont act on services without [Install] sections
ExecStart=/usr/bin/bash -c '\
    systemctl reset-failed; \
    if test -e ${root_dir}/service_changed.req; then \
        changed=$(cat ${root_dir}/service_changed.req | \
            grep -v "^#" | grep -v "^[[:space:]]*$" | sort | uniq); \
        if test "$changed" != ""; then \
            torestart=""; \
            templates=""; \
            for i in $changed; do \
                if test "$(echo $i | sed -r "s/([^@]+)@.*/\\1/g")" != "$i"; then \
                    templates="$templates $i"; \
                else \
                    wantedby=$(systemctl show "$i.service" --property=WantedBy --value); \
                    if test "$wantedby" != ""; then \
                        torestart="$torestart $i"; \
                    fi; \
                fi; \
            done; \
            if test "$torestart" != ""; then \
                echo "Restarting: $torestart"; \
                systemctl restart $torestart; \
            fi; \
            if test "$templates" != ""; then \
                echo "Information: Templates changed: $templates"; \
            fi; \
        fi; \
    fi'

# delete main.sls and var/cache because of secrets and as flag that update is done
ExecStart=/usr/bin/bash -c 'rm -f ${sls_dir}/main.sls; rm -rf ${root_dir}/var/cache'

# XXX have no [Install] section, because this service is only called from external
